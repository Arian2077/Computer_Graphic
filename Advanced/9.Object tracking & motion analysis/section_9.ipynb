{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f51c6e29",
   "metadata": {},
   "source": [
    "# Object tracking & motion analysis\n",
    "\n",
    "## فیلتر کردن رنگ\n",
    "\n",
    "### فیلتر بر اساس Hue (فام)\n",
    "\n",
    "- بازه رنگ Hue از ۰ تا ۱۸۰ است\n",
    "\n",
    "- بازه‌های فیلتر رنگ:\n",
    "    - قرمز: ۱۶۵ تا ۱۵\n",
    "    - سبز: ۴۵ تا ۷۵\n",
    "    - آبی: ۹۰ تا ۱۲۰\n",
    "\n",
    "### فیلتر بر اساس اشباع (Saturation) و روشنایی (Value/Brightness)\n",
    "\n",
    "- معمولاً بازه فیلتر را برای اشباع و روشنایی از ۵۰ تا ۲۵۵ قرار می‌دهیم تا رنگ‌های واقعی را پوشش دهد\n",
    "- بازه ۰ تا ۶۰ در اشباع نزدیک به سفید است\n",
    "- بازه ۰ تا ۶۰ در روشنایی نزدیک به سیاه است\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e181c87",
   "metadata": {},
   "source": [
    "## Filter by color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd2572c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# define range of PURPLE color in HSV\n",
    "lower_purple = np.array([125,0,0])\n",
    "upper_purple = np.array([175,255,255])\n",
    "\n",
    "# loop until break statement is exectured\n",
    "while True:\n",
    "    \n",
    "    # Read webcam image\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Convert image from RBG/BGR to HSV so we easily filter\n",
    "    hsv_img = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "\n",
    "    # Use inRange to capture only the values between lower & upper_blue\n",
    "    mask = cv2.inRange(hsv_img, lower_blue, upper_blue)\n",
    "\n",
    "    # Perform Bitwise AND on mask and our original frame\n",
    "    res = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "\n",
    "    cv2.imshow('Original', frame)  \n",
    "    cv2.imshow('mask', mask)\n",
    "    cv2.imshow('Filtered Color Only', res)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "        \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1243a5e",
   "metadata": {},
   "source": [
    "این کد برای فیلتر کردن یک رنگ خاص (در اینجا بنفش) از تصویر دریافتی وب‌کم استفاده می‌کند. مراحل کار به شرح زیر است:\n",
    "\n",
    "- ابتدا کتابخانه‌های مورد نیاز (`cv2` و `numpy`) وارد می‌شوند.\n",
    "- وب‌کم با استفاده از `cv2.VideoCapture(0)` فعال می‌شود.\n",
    "- محدوده رنگ بنفش در فضای رنگی HSV با دو آرایه `lower_purple` و `upper_purple` تعریف می‌شود.\n",
    "- یک حلقه بی‌نهایت اجرا می‌شود تا زمانی که کلید Enter فشرده شود.\n",
    "- در هر تکرار حلقه:\n",
    "    - یک فریم از وب‌کم خوانده می‌شود.\n",
    "    - تصویر از فضای رنگی BGR به HSV تبدیل می‌شود تا فیلتر کردن رنگ راحت‌تر انجام شود.\n",
    "    - با استفاده از تابع `cv2.inRange`، ماسکی ساخته می‌شود که فقط پیکسل‌هایی که در محدوده رنگ بنفش قرار دارند را نگه می‌دارد.\n",
    "    - با استفاده از عملگر بیت‌به‌بیت AND، فقط بخش‌هایی از تصویر که رنگ بنفش دارند نمایش داده می‌شوند.\n",
    "    - سه پنجره نمایش داده می‌شود: تصویر اصلی، ماسک، و تصویر فیلتر شده.\n",
    "    - اگر کلید Enter فشرده شود، حلقه متوقف می‌شود.\n",
    "- در پایان، وب‌کم آزاد شده و تمام پنجره‌ها بسته می‌شوند.\n",
    "\n",
    "**نکته:** در کد نمونه، متغیرهای `lower_blue` و `upper_blue` به اشتباه استفاده شده‌اند و باید به `lower_purple` و `upper_purple` تغییر داده شوند.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9ff43a",
   "metadata": {},
   "source": [
    "## Background Subtraction\n",
    "\n",
    "این یک تکنیک بسیار مفید در بینایی ماشین است که به ما اجازه می‌دهد اجسام متحرک (پیش‌زمینه) را از پس‌زمینه در یک جریان ویدیویی جدا کنیم.\n",
    "\n",
    "این الگوریتم‌ها اساساً درباره فریم موجود در تصویر (جریان ویدیو) یاد می‌گیرند و می‌توانند به‌طور دقیق ماسک پیش‌زمینه را شناسایی کنند. نتیجه این کار، یک تقسیم‌بندی دودویی از تصویر است که نواحی اجسام غیرثابت را برجسته می‌کند.\n",
    "\n",
    "در OpenCV چندین الگوریتم تفریق پس‌زمینه مخصوص تحلیل ویدیو وجود دارد:\n",
    "\n",
    "- **BackgroundSubtractorMOG** – یک الگوریتم جداسازی پیش‌زمینه/پس‌زمینه مبتنی بر مخلوط گاوسی. هر پیکسل پس‌زمینه را با یک مخلوطی از توزیع‌های گاوسی مدل می‌کند و با یک تقریب آنلاین مدل را به‌روزرسانی می‌کند.\n",
    "- **BackgroundSubtractorMOG2** – نسخه بهبود یافته MOG که سازگاری بهتری با صحنه‌های متغیر و سایه‌ها دارد.\n",
    "- **BackgroundSubtractorGMG** – از تخمین آماری تصویر پس‌زمینه و تقسیم‌بندی بیزی پیکسل‌به‌پیکسل استفاده می‌کند.\n",
    "\n",
    "این الگوریتم‌ها به طور گسترده در کاربردهایی مانند ردیابی اشیا، تشخیص حرکت و نظارت تصویری استفاده می‌شوند.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0889d6de",
   "metadata": {},
   "source": [
    "### 1. Gaussian Mixture-based Background/Foreground Segmentation Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bffd085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV 2.4.13 only\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('walking.avi')\n",
    "\n",
    "# Initlaize background subtractor\n",
    "foreground_background = cv2.BackgroundSubtractorMOG()\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Apply background subtractor to get our foreground mask\n",
    "    foreground_mask = foreground_background.apply(frame)\n",
    "\n",
    "    cv2.imshow('Output', foreground_mask)\n",
    "    if cv2.waitKey(1) == 13: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f654987",
   "metadata": {},
   "source": [
    "این کد برای جداسازی پیش‌زمینه و پس‌زمینه در یک ویدیو با استفاده از الگوریتم مخلوط گاوسی (MOG) در OpenCV نسخه 2.4.13 نوشته شده است. توضیح مراحل به شرح زیر است:\n",
    "\n",
    "- ابتدا کتابخانه‌های مورد نیاز (`numpy` و `cv2`) وارد می‌شوند.\n",
    "- ویدیوی `'walking.avi'` با استفاده از `cv2.VideoCapture` بارگذاری می‌شود.\n",
    "- یک شیء از کلاس `BackgroundSubtractorMOG` ساخته می‌شود تا مدل پس‌زمینه را ایجاد کند.\n",
    "- یک حلقه بی‌نهایت اجرا می‌شود:\n",
    "    - هر بار یک فریم از ویدیو خوانده می‌شود.\n",
    "    - با استفاده از متد `apply`، ماسک پیش‌زمینه استخراج می‌شود؛ یعنی بخش‌هایی از تصویر که متحرک هستند جدا می‌شوند.\n",
    "    - ماسک به دست آمده نمایش داده می‌شود.\n",
    "    - اگر کلید Enter فشرده شود، حلقه متوقف می‌شود.\n",
    "- در پایان، ویدیو آزاد شده و تمام پنجره‌ها بسته می‌شوند.\n",
    "\n",
    "این روش برای تشخیص حرکت و ردیابی اشیاء متحرک در ویدیو کاربرد دارد.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438baf02",
   "metadata": {},
   "source": [
    "### اگر بخواهیم این روش را روی ورودی وب‌کم خود اجرا کنیم چه؟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b253910",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV 2.4.13 only\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Intialize Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initlaize background subtractor\n",
    "foreground_background = cv2.BackgroundSubtractorMOG()\n",
    "\n",
    "while True:\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Apply background subtractor to get our foreground mask\n",
    "    foreground_mask = foreground_background.apply(frame)\n",
    "\n",
    "    cv2.imshow('Output', foreground_mask)\n",
    "    if cv2.waitKey(1) == 13: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a45f2",
   "metadata": {},
   "source": [
    "این کد برای جداسازی پیش‌زمینه و پس‌زمینه در تصویر دریافتی از وب‌کم با استفاده از الگوریتم مخلوط گاوسی (MOG) در OpenCV نسخه 2.4.13 نوشته شده است. مراحل کار به شرح زیر است:\n",
    "\n",
    "- ابتدا کتابخانه‌های مورد نیاز (`numpy` و `cv2`) وارد می‌شوند.\n",
    "- وب‌کم با استفاده از `cv2.VideoCapture(0)` فعال می‌شود.\n",
    "- یک شیء از کلاس `BackgroundSubtractorMOG` ساخته می‌شود تا مدل پس‌زمینه را ایجاد کند.\n",
    "- یک حلقه بی‌نهایت اجرا می‌شود:\n",
    "    - هر بار یک فریم از وب‌کم خوانده می‌شود.\n",
    "    - با استفاده از متد `apply`، ماسک پیش‌زمینه استخراج می‌شود؛ یعنی بخش‌هایی از تصویر که متحرک هستند جدا می‌شوند.\n",
    "    - ماسک به دست آمده نمایش داده می‌شود.\n",
    "    - اگر کلید Enter فشرده شود، حلقه متوقف می‌شود.\n",
    "- در پایان، وب‌کم آزاد شده و تمام پنجره‌ها بسته می‌شوند.\n",
    "\n",
    "این روش برای تشخیص حرکت و ردیابی اشیاء متحرک در تصویر وب‌کم کاربرد دارد.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f115fec",
   "metadata": {},
   "source": [
    "### بیایید مدل مخلوط گاوسی تطبیقی بهبود یافته را برای تفریق پس‌زمینه امتحان کنیم"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b6b5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV 2.4.13\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture('walking.avi')\n",
    "\n",
    "# Initlaize background subtractor\n",
    "foreground_background = cv2.BackgroundSubtractorMOG2()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Apply background subtractor to get our foreground mask\n",
    "    foreground_mask = foreground_background.apply(frame)\n",
    "\n",
    "    cv2.imshow('Output', foreground_mask)\n",
    "    if cv2.waitKey(1) == 13: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963ae2d1",
   "metadata": {},
   "source": [
    "این کد برای جداسازی پیش‌زمینه و پس‌زمینه در یک ویدیو با استفاده از الگوریتم مخلوط گاوسی تطبیقی (MOG2) در OpenCV نسخه 2.4.13 نوشته شده است. مراحل کار به شرح زیر است:\n",
    "\n",
    "- ابتدا کتابخانه‌های مورد نیاز (`numpy` و `cv2`) وارد می‌شوند.\n",
    "- ویدیوی `'walking.avi'` با استفاده از `cv2.VideoCapture` بارگذاری می‌شود.\n",
    "- یک شیء از کلاس `BackgroundSubtractorMOG2` ساخته می‌شود تا مدل پس‌زمینه را ایجاد کند.\n",
    "- یک حلقه بی‌نهایت اجرا می‌شود:\n",
    "    - هر بار یک فریم از ویدیو خوانده می‌شود.\n",
    "    - با استفاده از متد `apply`، ماسک پیش‌زمینه استخراج می‌شود؛ یعنی بخش‌هایی از تصویر که متحرک هستند جدا می‌شوند.\n",
    "    - ماسک به دست آمده نمایش داده می‌شود.\n",
    "    - اگر کلید Enter فشرده شود، حلقه متوقف می‌شود.\n",
    "- در پایان، ویدیو آزاد شده و تمام پنجره‌ها بسته می‌شوند.\n",
    "\n",
    "این روش برای تشخیص حرکت و ردیابی اشیاء متحرک در ویدیو کاربرد دارد و نسبت به نسخه قبلی (MOG) عملکرد بهتری در شرایط نوری متغیر و سایه‌ها دارد.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10bfada8",
   "metadata": {},
   "source": [
    "### اعمال آن بر روی جریان وب‌کم خود"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0a2f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV 2.4.13\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Intialize Webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initlaize background subtractor\n",
    "foreground_background = cv2.BackgroundSubtractorMOG2()\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # Apply background subtractor to get our foreground mask\n",
    "    foreground_mask = foreground_background.apply(frame)\n",
    "\n",
    "    cv2.imshow('Output', foreground_mask)\n",
    "    if cv2.waitKey(1) == 13: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a45725",
   "metadata": {},
   "source": [
    "این کد برای جداسازی پیش‌زمینه و پس‌زمینه در تصویر دریافتی از وب‌کم با استفاده از الگوریتم مخلوط گاوسی تطبیقی (MOG2) در OpenCV نسخه 2.4.13 نوشته شده است. مراحل کار به شرح زیر است:\n",
    "\n",
    "- ابتدا کتابخانه‌های مورد نیاز (`numpy` و `cv2`) وارد می‌شوند.\n",
    "- وب‌کم با استفاده از `cv2.VideoCapture(0)` فعال می‌شود.\n",
    "- یک شیء از کلاس `BackgroundSubtractorMOG2` ساخته می‌شود تا مدل پس‌زمینه را ایجاد کند.\n",
    "- یک حلقه بی‌نهایت اجرا می‌شود:\n",
    "    - هر بار یک فریم از وب‌کم خوانده می‌شود.\n",
    "    - با استفاده از متد `apply`، ماسک پیش‌زمینه استخراج می‌شود؛ یعنی بخش‌هایی از تصویر که متحرک هستند جدا می‌شوند.\n",
    "    - ماسک به دست آمده نمایش داده می‌شود.\n",
    "    - اگر کلید Enter فشرده شود، حلقه متوقف می‌شود.\n",
    "- در پایان، وب‌کم آزاد شده و تمام پنجره‌ها بسته می‌شوند.\n",
    "\n",
    "این روش برای تشخیص حرکت و ردیابی اشیاء متحرک در تصویر وب‌کم کاربرد دارد و نسبت به نسخه قبلی (MOG) عملکرد بهتری در شرایط نوری متغیر و سایه‌ها دارد.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657f1f8a",
   "metadata": {},
   "source": [
    "## تفریق پیش‌زمینه چیست؟"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3d139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Initalize webacam and store first frame\n",
    "cap = cv2.VideoCapture(0)\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# Create a flaot numpy array with frame values\n",
    "average = np.float32(frame)\n",
    "\n",
    "while True:\n",
    "    # Get webcam frmae\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    # 0.01 is the weight of image, play around to see how it changes\n",
    "    cv2.accumulateWeighted(frame, average, 0.01)\n",
    "    \n",
    "    # Scales, calculates absolute values, and converts the result to 8-bit\n",
    "    background = cv2.convertScaleAbs(average)\n",
    "\n",
    "    cv2.imshow('Input', frame)\n",
    "    cv2.imshow('Disapearing Background', background)\n",
    "    \n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad6f0f0",
   "metadata": {},
   "source": [
    "این کد برای حذف تدریجی پس‌زمینه از تصویر دریافتی وب‌کم با استفاده از میانگین‌گیری وزنی (Background Averaging) نوشته شده است. توضیح مراحل به شرح زیر است:\n",
    "\n",
    "- ابتدا کتابخانه‌های مورد نیاز (`cv2` و `numpy`) وارد می‌شوند.\n",
    "- وب‌کم با استفاده از `cv2.VideoCapture(0)` فعال می‌شود و اولین فریم خوانده می‌شود.\n",
    "- یک آرایه‌ی شناور (`float32`) با مقادیر اولین فریم ساخته می‌شود تا به عنوان میانگین اولیه پس‌زمینه استفاده شود.\n",
    "- یک حلقه بی‌نهایت اجرا می‌شود:\n",
    "    - در هر تکرار، یک فریم جدید از وب‌کم خوانده می‌شود.\n",
    "    - با استفاده از تابع `cv2.accumulateWeighted`، میانگین وزنی بین فریم فعلی و میانگین قبلی محاسبه می‌شود. پارامتر ۰.۰۱ وزن فریم جدید را تعیین می‌کند (هرچه این عدد بزرگ‌تر باشد، پس‌زمینه سریع‌تر به‌روزرسانی می‌شود).\n",
    "    - با استفاده از `cv2.convertScaleAbs`، میانگین به تصویر ۸ بیتی تبدیل می‌شود تا قابل نمایش باشد.\n",
    "    - دو پنجره نمایش داده می‌شود: یکی تصویر ورودی و دیگری پس‌زمینه‌ی محو شونده.\n",
    "    - اگر کلید Enter فشرده شود، حلقه متوقف می‌شود.\n",
    "- در پایان، تمام پنجره‌ها بسته شده و وب‌کم آزاد می‌شود.\n",
    "\n",
    "این روش برای حذف تدریجی اجسام متحرک و نمایش پس‌زمینه‌ی ثابت کاربرد دارد.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be55e12",
   "metadata": {},
   "source": [
    "### Background Substraction KKN\n",
    "#### OpenCV 3.X only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c0feb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV 3.1.0\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE,(3,3))\n",
    "fgbg = cv2.createBackgroundSubtractorKNN()\n",
    "\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    fgmask = fgbg.apply(frame)\n",
    "    fgmask = cv2.morphologyEx(fgmask, cv2.MORPH_OPEN, kernel)\n",
    "\n",
    "    cv2.imshow('frame',fgmask)\n",
    "    \n",
    "    if cv2.waitKey(1) == 13: \n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7825f6c",
   "metadata": {},
   "source": [
    "این کد برای جداسازی پیش‌زمینه و پس‌زمینه در تصویر دریافتی از وب‌کم با استفاده از الگوریتم KNN (K-Nearest Neighbors) در OpenCV نسخه 3.1.0 نوشته شده است. مراحل کار به شرح زیر است:\n",
    "\n",
    "- ابتدا کتابخانه‌های مورد نیاز (`numpy` و `cv2`) وارد می‌شوند.\n",
    "- وب‌کم با استفاده از `cv2.VideoCapture(0)` فعال می‌شود.\n",
    "- یک کرنل بیضوی با اندازه ۳x۳ برای عملیات مورفولوژیکی ساخته می‌شود.\n",
    "- شیء `createBackgroundSubtractorKNN` برای تفریق پس‌زمینه با الگوریتم KNN ساخته می‌شود.\n",
    "- یک حلقه بی‌نهایت اجرا می‌شود:\n",
    "    - هر بار یک فریم از وب‌کم خوانده می‌شود.\n",
    "    - با استفاده از متد `apply`، ماسک پیش‌زمینه استخراج می‌شود؛ یعنی بخش‌هایی از تصویر که متحرک هستند جدا می‌شوند.\n",
    "    - با استفاده از عملیات مورفولوژیکی `MORPH_OPEN`، نویزهای کوچک از ماسک حذف می‌شوند تا تصویر تمیزتری به دست آید.\n",
    "    - ماسک نهایی نمایش داده می‌شود.\n",
    "    - اگر کلید Enter فشرده شود، حلقه متوقف می‌شود.\n",
    "- در پایان، وب‌کم آزاد شده و تمام پنجره‌ها بسته می‌شوند.\n",
    "\n",
    "این روش برای تشخیص حرکت و ردیابی اشیاء متحرک در تصویر وب‌کم کاربرد دارد و نسبت به روش‌های قبلی در برخی شرایط عملکرد بهتری دارد.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2f19ad",
   "metadata": {},
   "source": [
    "## Meanshift - An Object tracking algorithm (میان‌شیفت - یک الگوریتم ردیابی شیء)\n",
    "این الگوریتم بر پایه یک ایده ساده بنا شده است: با یافتن بیشترین تراکم نقاط نمونه‌برداری شده و بازمحاسبه آن در فریم بعدی، پنجره مشاهده ما به سمت حرکت شیء جابجا می‌شود.\n",
    "\n",
    "در OpenCV معمولاً از تصویر back projection هیستوگرام و موقعیت اولیه هدف برای ردیابی استفاده می‌شود."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c10ac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# take first frame of the video\n",
    "ret, frame = cap.read()\n",
    "print(type(frame))\n",
    "\n",
    "# setup default location of window\n",
    "r, h, c, w = 240, 100, 400, 160 \n",
    "track_window = (c, r, w, h)\n",
    "\n",
    "# Crop region of interest for tracking\n",
    "roi = frame[r:r+h, c:c+w]\n",
    "\n",
    "# Convert cropped window to HSV color space\n",
    "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Create a mask between the HSV bounds\n",
    "lower_purple = np.array([125,0,0])\n",
    "upper_purple = np.array([175,255,255])\n",
    "mask = cv2.inRange(hsv_roi, lower_purple, upper_purple)\n",
    "\n",
    "# Obtain the color histogram of the ROI\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], mask, [180], [0,180])\n",
    "\n",
    "# Normalize values to lie between the range 0, 255\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# Setup the termination criteria\n",
    "# We stop calculating the centroid shift after ten iterations \n",
    "# or if the centroid has moved at least 1 pixel\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # Read webcam frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "        \n",
    "        # Convert to HSV\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Calculate the histogram back projection \n",
    "        # Each pixel's value is it's probability\n",
    "        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "\n",
    "        # apply meanshift to get the new location\n",
    "        ret, track_window = cv2.meanShift(dst, track_window, term_crit)\n",
    "\n",
    "        # Draw it on image\n",
    "        x, y, w, h = track_window\n",
    "        img2 = cv2.rectangle(frame, (x,y), (x+w, y+h), 255, 2)    \n",
    "\n",
    "        cv2.imshow('Meansift Tracking', img2)\n",
    "        \n",
    "        if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73982194",
   "metadata": {},
   "source": [
    "این کد یک پیاده‌سازی از الگوریتم میان‌شیفت (Meanshift) برای ردیابی شیء در تصویر دریافتی از وب‌کم است. مراحل کار به شرح زیر است:\n",
    "\n",
    "- ابتدا کتابخانه‌های مورد نیاز (`numpy` و `cv2`) وارد می‌شوند.\n",
    "- وب‌کم با استفاده از `cv2.VideoCapture(0)` فعال می‌شود.\n",
    "- اولین فریم از وب‌کم خوانده می‌شود تا ناحیه اولیه ردیابی انتخاب شود.\n",
    "- مختصات و ابعاد پنجره ردیابی (`track_window`) تعیین می‌شود و ناحیه مورد نظر (ROI) از تصویر بریده می‌شود.\n",
    "- ناحیه انتخابی به فضای رنگی HSV تبدیل می‌شود تا فیلتر رنگ راحت‌تر انجام شود.\n",
    "- یک ماسک برای محدوده رنگ بنفش ایجاد می‌شود تا فقط پیکسل‌های این رنگ در ناحیه مورد نظر باقی بمانند.\n",
    "- هیستوگرام رنگی ناحیه انتخابی محاسبه و نرمال‌سازی می‌شود تا برای ردیابی استفاده شود.\n",
    "- معیار توقف الگوریتم میان‌شیفت تعیین می‌شود (۱۰ تکرار یا جابجایی کمتر از ۱ پیکسل).\n",
    "- در یک حلقه بی‌نهایت:\n",
    "    - هر بار یک فریم جدید از وب‌کم خوانده می‌شود.\n",
    "    - تصویر به فضای HSV تبدیل می‌شود.\n",
    "    - تصویر back projection هیستوگرام محاسبه می‌شود تا احتمال حضور شیء در هر نقطه مشخص شود.\n",
    "    - الگوریتم میان‌شیفت روی تصویر back projection اجرا می‌شود تا موقعیت جدید پنجره ردیابی به دست آید.\n",
    "    - یک مستطیل روی شیء ردیابی‌شده رسم و نمایش داده می‌شود.\n",
    "    - اگر کلید Enter فشرده شود، حلقه متوقف می‌شود.\n",
    "- در پایان، تمام پنجره‌ها بسته شده و وب‌کم آزاد می‌شود.\n",
    "\n",
    "این کد برای ردیابی اشیاء رنگی (در اینجا بنفش) در تصویر ویدیویی کاربرد دارد و با استفاده از الگوریتم میان‌شیفت موقعیت شیء را در هر فریم به‌روزرسانی می‌کند.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e42072c",
   "metadata": {},
   "source": [
    "## Camshift - An object tracking Algorithm (کم‌شیفت - یک الگوریتم ردیابی شیء)\n",
    "الگوریتم Camshift بسیار شبیه به الگوریتم Meanshift است، اما شاید متوجه شده باشید که پنجره در Meanshift اندازه ثابتی دارد. این موضوع می‌تواند مشکل‌ساز باشد، زیرا حرکت اشیا در تصویر می‌تواند کوچک یا بزرگ باشد و اگر پنجره بیش از حد بزرگ باشد، ممکن است شیء هنگام ردیابی از دست برود.\n",
    "\n",
    "Camshift (مخفف continuously adaptive Meanshift) از یک پنجره تطبیقی استفاده می‌کند که اندازه و حتی جهت آن (یعنی می‌تواند بچرخد) تغییر می‌کند. مراحل ساده‌شده این الگوریتم به شرح زیر است:\n",
    "\n",
    "1. اجرای Meanshift تا زمانی که همگرا شود\n",
    "2. محاسبه اندازه پنجره\n",
    "3. محاسبه جهت با استفاده از بهترین بیضی برازش‌یافته\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593d82fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Initialize webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# take first frame of the video\n",
    "ret, frame = cap.read()\n",
    "\n",
    "# setup default location of window\n",
    "r, h, c, w = 240, 100, 400, 160 \n",
    "track_window = (c, r, w, h)\n",
    "\n",
    "# Crop region of interest for tracking\n",
    "roi = frame[r:r+h, c:c+w]\n",
    "\n",
    "# Convert cropped window to HSV color space\n",
    "hsv_roi =  cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "# Create a mask between the HSV bounds\n",
    "lower_purple = np.array([130,60,60])\n",
    "upper_purple = np.array([175,255,255])\n",
    "mask = cv2.inRange(hsv_roi, lower_purple, upper_purple)\n",
    "\n",
    "# Obtain the color histogram of the ROI\n",
    "roi_hist = cv2.calcHist([hsv_roi], [0], mask, [180], [0,180])\n",
    "\n",
    "# Normalize values to lie between the range 0, 255\n",
    "cv2.normalize(roi_hist, roi_hist, 0, 255, cv2.NORM_MINMAX)\n",
    "\n",
    "# Setup the termination criteria\n",
    "# We stop calculating the centroid shift after ten iterations \n",
    "# or if the centroid has moved at least 1 pixel\n",
    "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
    "\n",
    "while True:\n",
    "    \n",
    "    # Read webcam frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret == True:\n",
    "        # Convert to HSV\n",
    "        hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "        \n",
    "        # Calculate the histogram back projection \n",
    "        # Each pixel's value is it's probability\n",
    "        dst = cv2.calcBackProject([hsv],[0],roi_hist,[0,180],1)\n",
    "\n",
    "        # apply Camshift to get the new location\n",
    "        ret, track_window = cv2.CamShift(dst, track_window, term_crit)\n",
    "\n",
    "        # Draw it on image \n",
    "        # We use polylines to represent Adaptive box \n",
    "        pts = cv2.boxPoints(ret)\n",
    "        pts = np.int0(pts)\n",
    "        img2 = cv2.polylines(frame,[pts],True, 255,2)\n",
    "        \n",
    "        cv2.imshow('Camshift Tracking', img2)\n",
    "        \n",
    "        if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "            break\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ad650d",
   "metadata": {},
   "source": [
    "این کد پیاده‌سازی الگوریتم Camshift برای ردیابی شیء در تصویر دریافتی از وب‌کم است. مراحل کار به شرح زیر است:\n",
    "\n",
    "- ابتدا کتابخانه‌های مورد نیاز (`numpy` و `cv2`) وارد می‌شوند.\n",
    "- وب‌کم با استفاده از `cv2.VideoCapture(0)` فعال می‌شود.\n",
    "- اولین فریم از وب‌کم خوانده می‌شود تا ناحیه اولیه ردیابی انتخاب شود.\n",
    "- مختصات و ابعاد پنجره ردیابی (`track_window`) تعیین می‌شود و ناحیه مورد نظر (ROI) از تصویر بریده می‌شود.\n",
    "- ناحیه انتخابی به فضای رنگی HSV تبدیل می‌شود تا فیلتر رنگ راحت‌تر انجام شود.\n",
    "- یک ماسک برای محدوده رنگ بنفش ایجاد می‌شود تا فقط پیکسل‌های این رنگ در ناحیه مورد نظر باقی بمانند.\n",
    "- هیستوگرام رنگی ناحیه انتخابی محاسبه و نرمال‌سازی می‌شود تا برای ردیابی استفاده شود.\n",
    "- معیار توقف الگوریتم Camshift تعیین می‌شود (۱۰ تکرار یا جابجایی کمتر از ۱ پیکسل).\n",
    "- در یک حلقه بی‌نهایت:\n",
    "    - هر بار یک فریم جدید از وب‌کم خوانده می‌شود.\n",
    "    - تصویر به فضای HSV تبدیل می‌شود.\n",
    "    - تصویر back projection هیستوگرام محاسبه می‌شود تا احتمال حضور شیء در هر نقطه مشخص شود.\n",
    "    - الگوریتم Camshift روی تصویر back projection اجرا می‌شود تا موقعیت و اندازه جدید پنجره ردیابی به دست آید.\n",
    "    - یک چندضلعی (مستطیل تطبیقی) روی شیء ردیابی‌شده رسم و نمایش داده می‌شود.\n",
    "    - اگر کلید Enter فشرده شود، حلقه متوقف می‌شود.\n",
    "- در پایان، تمام پنجره‌ها بسته شده و وب‌کم آزاد می‌شود.\n",
    "\n",
    "این کد برای ردیابی اشیاء رنگی (در اینجا بنفش) در تصویر ویدیویی کاربرد دارد و با استفاده از الگوریتم Camshift موقعیت و اندازه پنجره ردیابی را در هر فریم به‌روزرسانی می‌کند.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe4877e",
   "metadata": {},
   "source": [
    "## When and how to use Meanshift or Camshift (چه زمانی و چگونه از Meanshift یا Camshift استفاده کنیم)\n",
    "\n",
    "انتخاب بین الگوریتم Meanshift و Camshift بستگی به ویژگی‌های شیء مورد ردیابی و شرایط صحنه دارد:\n",
    "\n",
    "- **Meanshift** زمانی مناسب است که:\n",
    "    - اندازه و شکل شیء مورد ردیابی در طول زمان تقریباً ثابت بماند.\n",
    "    - اطلاعات اولیه دقیقی از موقعیت و ابعاد شیء دارید.\n",
    "    - نیازی به تغییر اندازه یا چرخش پنجره ردیابی ندارید.\n",
    "\n",
    "- **Camshift** زمانی مناسب است که:\n",
    "    - اندازه، شکل یا جهت شیء در طول زمان تغییر می‌کند (مثلاً شیء به دوربین نزدیک یا از آن دور می‌شود).\n",
    "    - می‌خواهید پنجره ردیابی به صورت خودکار با تغییرات شیء تطبیق پیدا کند.\n",
    "    - انعطاف‌پذیری بیشتری برای ردیابی اشیاء متحرک و تغییرپذیر نیاز دارید.\n",
    "\n",
    "**نکته:** تعیین صحیح موقعیت و اندازه اولیه پنجره ردیابی بسیار مهم است. اگر این مقدار به‌درستی انتخاب نشود، هر دو الگوریتم ممکن است به جای شیء اصلی، یک ناحیه اشتباه را دنبال کنند یا در یک کمینه محلی گیر کنند."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4033cd",
   "metadata": {},
   "source": [
    "## Optical Flow (جریان نوری)\n",
    "به‌دنبال به‌دست آوردن الگوی حرکت ظاهری اشیاء در یک تصویر بین دو فریم متوالی است.\n",
    "توزیع سرعت‌های ظاهری اشیاء در یک تصویر را نمایش می‌دهد.\n",
    "## optical flow implementations in Opencv (پیاده‌سازی‌های جریان نوری در OpenCV)\n",
    "در OpenCV دو پیاده‌سازی برای optical flow وجود دارد:\n",
    "- روش تفاضلی Lucas-Kanade – برخی نقاط کلیدی را در ویدیو دنبال می‌کند و برای ویژگی‌های گوشه‌دار مناسب است (مثلاً ردیابی خودروها از نمای پهپاد).\n",
    "- optical flow چگال (dense optical flow) – کندتر است اما جریان نوری را برای همه نقاط فریم محاسبه می‌کند، برخلاف Lucas-Kanade که فقط از ویژگی‌های گوشه‌ای (مجموعه داده پراکنده) استفاده می‌کند. در این روش، رنگ‌ها برای نمایش حرکت به‌کار می‌روند؛ Hue جهت حرکت و Value (روشنایی/شدت) سرعت را نشان می‌دهد.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06a009d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# Load video stream\n",
    "cap = cv2.VideoCapture('images/walking.avi')\n",
    "\n",
    "# Set parameters for ShiTomasi corner detection\n",
    "feature_params = dict( maxCorners = 100,\n",
    "                       qualityLevel = 0.3,\n",
    "                       minDistance = 7,\n",
    "                       blockSize = 7 )\n",
    "\n",
    "# Set parameters for lucas kanade optical flow\n",
    "lucas_kanade_params = dict( winSize  = (15,15),\n",
    "                  maxLevel = 2,\n",
    "                  criteria = (cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 0.03))\n",
    "\n",
    "# Create some random colors\n",
    "# Used to create our trails for object movement in the image \n",
    "color = np.random.randint(0,255,(100,3))\n",
    "\n",
    "# Take first frame and find corners in it\n",
    "ret, prev_frame = cap.read()\n",
    "prev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Find inital corner locations\n",
    "prev_corners = cv2.goodFeaturesToTrack(prev_gray, mask = None, **feature_params)\n",
    "\n",
    "# Create a mask image for drawing purposes\n",
    "mask = np.zeros_like(prev_frame)\n",
    "\n",
    "while(1):\n",
    "    ret, frame = cap.read()\n",
    "    frame_gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # calculate optical flow\n",
    "    new_corners, status, errors = cv2.calcOpticalFlowPyrLK(prev_gray, \n",
    "                                                           frame_gray, \n",
    "                                                           prev_corners, \n",
    "                                                           None, \n",
    "                                                           **lucas_kanade_params)\n",
    "\n",
    "    # Select and store good points\n",
    "    good_new = new_corners[status==1]\n",
    "    good_old = prev_corners[status==1]\n",
    "\n",
    "    # Draw the tracks\n",
    "    for i,(new,old) in enumerate(zip(good_new, good_old)):\n",
    "        a, b = new.ravel()\n",
    "        c, d = old.ravel()\n",
    "        mask = cv2.line(mask, (a,b),(c,d), color[i].tolist(), 2)\n",
    "        frame = cv2.circle(frame, (a,b), 5, color[i].tolist(),-1)\n",
    "        \n",
    "    img = cv2.add(frame,mask)\n",
    "\n",
    "    # Show Optical Flow\n",
    "    cv2.imshow('Optical Flow - Lucas-Kanade',img)\n",
    "    if cv2.waitKey(1) == 13: #13 is the Enter Key\n",
    "        break\n",
    "\n",
    "    # Now update the previous frame and previous points\n",
    "    prev_gray = frame_gray.copy()\n",
    "    prev_corners = good_new.reshape(-1,1,2)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c482615",
   "metadata": {},
   "source": [
    "این کد پیاده‌سازی جریان نوری Lucas-Kanade برای ردیابی حرکت نقاط کلیدی در یک ویدیو است. مراحل کار به شرح زیر است:\n",
    "\n",
    "- ابتدا کتابخانه‌های مورد نیاز (`numpy` و `cv2`) وارد می‌شوند.\n",
    "- ویدیوی `'images/walking.avi'` بارگذاری می‌شود.\n",
    "- پارامترهای مورد نیاز برای شناسایی گوشه‌ها با الگوریتم Shi-Tomasi تعیین می‌شود.\n",
    "- پارامترهای الگوریتم Lucas-Kanade برای محاسبه جریان نوری تنظیم می‌شود.\n",
    "- ۱۰۰ رنگ تصادفی برای رسم مسیر حرکت نقاط تولید می‌شود.\n",
    "- اولین فریم ویدیو خوانده شده و به تصویر خاکستری تبدیل می‌شود.\n",
    "- نقاط گوشه‌ای (ویژگی‌های مناسب برای ردیابی) در اولین فریم شناسایی می‌شوند.\n",
    "- یک تصویر ماسک (برای رسم خطوط حرکت) با همان اندازه فریم ساخته می‌شود.\n",
    "- در یک حلقه:\n",
    "    - هر بار یک فریم جدید خوانده و به خاکستری تبدیل می‌شود.\n",
    "    - جریان نوری بین فریم قبلی و فعلی با استفاده از `cv2.calcOpticalFlowPyrLK` محاسبه می‌شود.\n",
    "    - نقاط خوب جدید و قدیمی انتخاب می‌شوند (نقاطی که با موفقیت ردیابی شده‌اند).\n",
    "    - برای هر نقطه، یک خط بین موقعیت قبلی و جدید رسم می‌شود و یک دایره روی نقطه جدید کشیده می‌شود.\n",
    "    - تصویر نهایی با جمع کردن ماسک و فریم فعلی ساخته می‌شود و نمایش داده می‌شود.\n",
    "    - اگر کلید Enter فشرده شود، حلقه متوقف می‌شود.\n",
    "    - فریم و نقاط فعلی برای تکرار بعدی به‌روزرسانی می‌شوند.\n",
    "- در پایان، تمام پنجره‌ها بسته شده و ویدیو آزاد می‌شود.\n",
    "\n",
    "این کد برای نمایش مسیر حرکت نقاط کلیدی در ویدیو و تحلیل حرکت اشیا کاربرد دارد.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
